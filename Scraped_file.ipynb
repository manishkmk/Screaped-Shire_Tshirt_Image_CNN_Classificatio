{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (3.141.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from selenium) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (0.23.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from scikit-learn) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\scrapro\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached pandas-1.0.5-cp37-cp37m-win_amd64.whl (8.7 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\scrapimg\\lib\\site-packages (from pandas) (1.19.0)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\envs\\scrapimg\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\scrapimg\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.0.5 pytz-2020.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path = os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} images\".format(index+1,len(data['image_urls'])))\n",
    "        response = requests.get(link)\n",
    "        with open ('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),\"wb\") as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data,filename):\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(filename,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sraping Images URLS Selenium\n",
    "def scrap_images_url(driver):\n",
    "    images = driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    product_data = {}\n",
    "    product_data['image_urls'] = []\n",
    "    \n",
    "    \n",
    "    for image in images:\n",
    "        source = image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "        print(\"Returing scraped data\")\n",
    "    \n",
    "    \n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver as wb\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH =r\"E:\\New folder (3)\\chromedriver.exe\"\n",
    "driver =wb.Chrome(executable_path=DRIVER_PATH)\n",
    "current_page_url = driver.get(\"https://www.flipkart.com/search?q=mens+shirt&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME=\"men_Tshirt\"\n",
    "\n",
    "make_directory(DIRNAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Returing scraped data\n",
      "Scraping page (0) fo (1)page\n",
      "The current page scaped is\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page (0) done\n"
     ]
    }
   ],
   "source": [
    "start_page=1\n",
    "total_pages=1\n",
    "#scraping the image\n",
    "for page in range(start_page,total_pages+1):\n",
    "    try:\n",
    "        product_details=scrap_images_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)page\".format(page,total_pages))\n",
    "        page_value=driver.find_element_by_xpath(\"//img[@class='_3togXc']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"men_shirt.csv\")\n",
    "       \n",
    "      \n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing a exception\")\n",
    "        exp_page=driver.find_element_by_path(\"//a[@class='_3togXc']\").text\n",
    "        print(\"The Page value at the time of exception is {}\".format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_path(\"//a[@class='_3ZJShS31bMyl']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page (0) fo (1)pages\".format(page,total_pages))\n",
    "        \n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_3ZJShS31bMyl']\").text\n",
    "        print(\"The current page scaped is{}\".format(page_value))\n",
    "    \n",
    "        ##Downloading the image\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"Scraping of page (0) done\".format(page))\n",
    "        #saving the data into a csv file_x\n",
    "        save_data_to_csv(data=product_details,filename=\"men_shirt.csv\")\n",
    "      \n",
    "        print('new_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
